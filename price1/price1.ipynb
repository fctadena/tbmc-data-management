{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. CREATE EXCEL FILE CALLED \"TBMC_ITEMS_TO_MONITOR.xlsx\" AS BASE SOURCE OF DATA FOR PRICE TO MONITOR, URL AND OTHER DATA.\n",
    "#2. DEVELOP THE APP\n",
    "#2.X SAVE/UPDATE THE RESULT TO AN EXCEL FILE CALLED \"PRICE_RECORDINGS.xlsx\"\n",
    "#3. USE AIRFLOW TO SCHEDULE THE TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from openpyxl import load_workbook\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.4'"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. DEVELOP THE APP\n",
    "#2.1 create necessary variables, flags & dataframe for scrapped data \"scrapped_data_df\"\n",
    "file_path = \"TBMC_ITEMS_TO_MONITOR.xlsx\"\n",
    "columns = ['description', 'type', 'url', 'price', 'date_stamp']\n",
    "scrapped_data_df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2.2 xlsx to dict - initialize the process by storing the required items (products to monitor) in dictionary called \"products\"\n",
    "def read_excel_file(file_path):\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found.\")\n",
    "        return {}\n",
    "    \n",
    "    expected_headers = [\"description\", \"type\", \"url\", \"identifier\"]\n",
    "    actual_headers = df.columns.tolist()\n",
    "    \n",
    "    if actual_headers != expected_headers:\n",
    "        print(\"Headers are not as expected.\")\n",
    "        return {}\n",
    "    \n",
    "    products = []\n",
    "    for index, row in df.iterrows():\n",
    "        description = row[\"description\"]\n",
    "        product_type = row[\"type\"]\n",
    "        url = row[\"url\"]\n",
    "        identifier = row[\"identifier\"]\n",
    "        \n",
    "        product = {\n",
    "            \"description\": description,\n",
    "            \"type\": product_type,\n",
    "            \"url\": url,\n",
    "            \"identifier\": identifier \n",
    "        }\n",
    "        \n",
    "        products.append(product)\n",
    "    \n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_checker(url, identifier):\n",
    "    # Initialize variables to store price and status\n",
    "    price = None\n",
    "    status = None\n",
    "    \n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find the element with the specified identifier\n",
    "            element = soup.find(id=identifier)\n",
    "            \n",
    "            if element:\n",
    "                # Extract the price from the element (assuming it's text)\n",
    "                price = element.text.strip()\n",
    "                status = 'Success'\n",
    "            else:\n",
    "                status = f'Element with identifier \"{identifier}\" not found'\n",
    "        else:\n",
    "            status = f'Request failed with status code {response.status_code}'\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        status = f'Request failed: {str(e)}'\n",
    "    \n",
    "    return {'price': price, 'status': status}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #2.3 main_processor - loop through the dict, perform scrapping, and add records to scrapped_data_df\n",
    "def core_func(products):\n",
    "    # columns = [\"description\", \"type\", \"url\", \"price\", \"time_stamp\", \"status\"]\n",
    "    columns = [\"description\", \"type\", \"url\", \"identifier\", \"price\", \"time_stamp\"]\n",
    "    scrapped_data_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in products:\n",
    "        #Generate \"price\", \"time_stamp\" and \"status\" here.!!!\n",
    "        current_time_stamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        price = 220 #sample only\n",
    "        \n",
    "        \n",
    "        new_row = [\n",
    "            i['description'],\n",
    "            i['type'],\n",
    "            i['url'],\n",
    "            i['identifier'],\n",
    "            price,\n",
    "            current_time_stamp\n",
    "        ]\n",
    "\n",
    "        scrapped_data_df.loc[len(scrapped_data_df)] = new_row\n",
    "        \n",
    "    print(scrapped_data_df.dtypes)\n",
    "    return scrapped_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.4 append (or create PRICE_RECORDINGS.xlsx) using the dataframe\n",
    "def update_price_recordings(scrapped_data_df):\n",
    "    file_name = \"PRICE_RECORDINGS_NEW1.xlsx\"\n",
    "    sheet_name = 'Sheet1'\n",
    "    \n",
    "    # # Convert columns to appropriate types\n",
    "    # scrapped_data_df['time_stamp'] = pd.to_datetime(scrapped_data_df['time_stamp'])\n",
    "    # scrapped_data_df['url'] = scrapped_data_df['url'].astype(str)\n",
    "    # scrapped_data_df['description'] = scrapped_data_df['description'].astype(str)\n",
    "    # scrapped_data_df['type'] = scrapped_data_df['type'].astype('category')\n",
    "    # # Add other conversions as needed\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_name):\n",
    "        # If the file does not exist, create it and write the scrapped_data_df to it\n",
    "        try:\n",
    "            with pd.ExcelWriter(file_name, engine='openpyxl') as writer:\n",
    "                scrapped_data_df.to_excel(writer, index=False, sheet_name=sheet_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating file: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        try:\n",
    "            # If the file exists, load it and append the data\n",
    "            book = load_workbook(file_name)\n",
    "            \n",
    "            # Ensure at least one sheet is visible\n",
    "            if all(sheet.sheet_state == 'hidden' for sheet in book.worksheets):\n",
    "                book.create_sheet(title=sheet_name)\n",
    "            \n",
    "            with pd.ExcelWriter(file_name, engine='openpyxl') as writer:\n",
    "                writer.book = book\n",
    "                writer.sheets = {ws.title: ws for ws in book.worksheets}\n",
    "                \n",
    "                # Check if the specified sheet exists\n",
    "                if sheet_name in writer.sheets:\n",
    "                    # Get the last row in the existing Excel sheet\n",
    "                    startrow = writer.sheets[sheet_name].max_row\n",
    "                else:\n",
    "                    startrow = 0\n",
    "                \n",
    "                # Append the data\n",
    "                scrapped_data_df.to_excel(writer, startrow=startrow, index=False, header=startrow==0, sheet_name=sheet_name)\n",
    "                \n",
    "                # Save and close the file\n",
    "                writer.save()\n",
    "        except Exception as e:\n",
    "            print(f\"Error appending data: {e}\")\n",
    "            return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description    object\n",
      "type           object\n",
      "url            object\n",
      "identifier     object\n",
      "price          object\n",
      "time_stamp     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#2.5 create main function \n",
    "def main():\n",
    "    # # 2.1 create necessary variables, flags & dataframe for scrapped data \"scrapped_data_df\"\n",
    "    # file_path = \"TBMC_ITEMS_TO_MONITOR.xlsx\"\n",
    "    # columns = ['description', 'type', 'url', 'price', 'date_stamp']\n",
    " \n",
    "\n",
    "    # scrapped_data_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # 2.2 xlsx to dict - initialize the process by storing the required items (products to monitor) in dictionary called \"products\"\n",
    "    products = read_excel_file(file_path)\n",
    "\n",
    "    # 2.3 main_processor - loop through the dict, perform scrapping, and add records to scrapped_data_df\n",
    "    scrapped_data_df = core_func(products)\n",
    "\n",
    "    # 2.4 append (or create PRICE_RECORDINGS.xlsx) using the dataframe\n",
    "    update_price_recordings(scrapped_data_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. USE AIRFLOW TO SCHEDULE THE TASK\n",
    "\n",
    "#OTHER NOTES TO CONSIDER\n",
    "# If getting the data on the url has failed, handle it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The warning about unreadable content in Excel files typically occurs when Excel detects something unexpected or inconsistent in the file structure. This can happen due to a few reasons:\n",
    "\n",
    "# Data Type Mismatch: Excel expects data types to be consistent within columns. If your DataFrame contains mixed data types in a column (e.g., numbers and text mixed), Excel might flag this as unreadable content.\n",
    "\n",
    "# File Corruption: Errors during file writing or closing can sometimes corrupt the file, leading to unreadable content warnings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
